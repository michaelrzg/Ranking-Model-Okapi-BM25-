URL: https://nlp.stanford.edu/IR-book/html/htmledition/feature-selection-1.html
Access Time: 2025-05-21 21:11:24.432678
Title: Feature selection
Text: 


Figure:
Basic feature selection algorithm for selecting the
 best features.




We can view feature selection as a method for replacing a
complex classifier (using all features) with a
simpler one (using a subset of the
features). It may appear counterintuitive at first that a
seemingly weaker classifier is advantageous in statistical text
classification,
but when discussing 
the bias-variance tradeoff in
Section 14.6 (page ), we will see
that weaker models are often preferable when limited
training data are available.


The basic feature selection algorithm is shown in
Figure 13.6 . 
For a given class ,
we compute a utility measure  for each
term of the vocabulary and select the  terms that have the highest values of . All
other terms are discarded and not used in classification. We will
introduce three different utility measures in this section: mutual information,

; the  test,

; and frequency,

.


Of the two NB models, the Bernoulli model is particularly
sensitive to noise features. A Bernoulli NB classifier
requires some form of feature selection or else its accuracy will
be low.


This section mainly addresses feature selection for two-class
classification tasks like
China versus
not-China. Section 13.5.5  briefly discusses
optimizations for systems with more than two
classes.




Subsections

Mutual
  information
 Feature selectionChi2 Feature selection

Assessing
     as a feature selection
    methodAssessing chi-square as a feature
    selection method


Frequency-based feature
  selection
Feature selection for multiple classifiers
Comparison of feature selection methods















 Next: Mutual information
 Up: Text classification and Naive
 Previous: A variant of the
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07



We can view feature selection as a method for replacing a
complex classifier (using all features) with a
simpler one (using a subset of the
features). It may appear counterintuitive at first that a
seemingly weaker classifier is advantageous in statistical text
classification,
but when discussing 
the bias-variance tradeoff in
Section 14.6 (page ), we will see
that weaker models are often preferable when limited
training data are available.


The basic feature selection algorithm is shown in
Figure 13.6 . 
For a given class ,
we compute a utility measure  for each
term of the vocabulary and select the  terms that have the highest values of . All
other terms are discarded and not used in classification. We will
introduce three different utility measures in this section: mutual information,

; the  test,

; and frequency,

.


Of the two NB models, the Bernoulli model is particularly
sensitive to noise features. A Bernoulli NB classifier
requires some form of feature selection or else its accuracy will
be low.


This section mainly addresses feature selection for two-class
classification tasks like
China versus
not-China. Section 13.5.5  briefly discusses
optimizations for systems with more than two
classes.




Subsections

Mutual
  information
 Feature selectionChi2 Feature selection

Assessing
     as a feature selection
    methodAssessing chi-square as a feature
    selection method


Frequency-based feature
  selection
Feature selection for multiple classifiers
Comparison of feature selection methods















 Next: Mutual information
 Up: Text classification and Naive
 Previous: A variant of the
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07



The basic feature selection algorithm is shown in
Figure 13.6 . 
For a given class ,
we compute a utility measure  for each
term of the vocabulary and select the  terms that have the highest values of . All
other terms are discarded and not used in classification. We will
introduce three different utility measures in this section: mutual information,

; the  test,

; and frequency,

.


Of the two NB models, the Bernoulli model is particularly
sensitive to noise features. A Bernoulli NB classifier
requires some form of feature selection or else its accuracy will
be low.


This section mainly addresses feature selection for two-class
classification tasks like
China versus
not-China. Section 13.5.5  briefly discusses
optimizations for systems with more than two
classes.




Subsections

Mutual
  information
 Feature selectionChi2 Feature selection

Assessing
     as a feature selection
    methodAssessing chi-square as a feature
    selection method


Frequency-based feature
  selection
Feature selection for multiple classifiers
Comparison of feature selection methods















 Next: Mutual information
 Up: Text classification and Naive
 Previous: A variant of the
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07



Of the two NB models, the Bernoulli model is particularly
sensitive to noise features. A Bernoulli NB classifier
requires some form of feature selection or else its accuracy will
be low.


This section mainly addresses feature selection for two-class
classification tasks like
China versus
not-China. Section 13.5.5  briefly discusses
optimizations for systems with more than two
classes.




Subsections

Mutual
  information
 Feature selectionChi2 Feature selection

Assessing
     as a feature selection
    methodAssessing chi-square as a feature
    selection method


Frequency-based feature
  selection
Feature selection for multiple classifiers
Comparison of feature selection methods















 Next: Mutual information
 Up: Text classification and Naive
 Previous: A variant of the
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07



This section mainly addresses feature selection for two-class
classification tasks like
China versus
not-China. Section 13.5.5  briefly discusses
optimizations for systems with more than two
classes.




Subsections

Mutual
  information
 Feature selectionChi2 Feature selection

Assessing
     as a feature selection
    methodAssessing chi-square as a feature
    selection method


Frequency-based feature
  selection
Feature selection for multiple classifiers
Comparison of feature selection methods















 Next: Mutual information
 Up: Text classification and Naive
 Previous: A variant of the
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07





Subsections

Mutual
  information
 Feature selectionChi2 Feature selection

Assessing
     as a feature selection
    methodAssessing chi-square as a feature
    selection method


Frequency-based feature
  selection
Feature selection for multiple classifiers
Comparison of feature selection methods















 Next: Mutual information
 Up: Text classification and Naive
 Previous: A variant of the
    Contents 
    Index


© 2008 Cambridge University PressThis is an automatically generated page. In case of formatting errors you may want to look at the PDF edition of the book.
2009-04-07


